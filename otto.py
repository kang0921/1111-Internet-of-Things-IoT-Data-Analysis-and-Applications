# -*- coding: utf-8 -*-
"""OTTO.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/kang0921/1111-Advanced-Data-Mining-and-Big-Data-Analysis/blob/main/OTTO.ipynb
"""

# Hyper Parameter Tuning
# 0:clicks 1:carts 2:orders
type_weight = {0:0.5,
               1:9,
               2:0.5}
type_weight_multipliers = type_weight

clicks_th = 15
carts_th  = 20 
orders_th = 20

VER = 7

"""## The following is an appropriation of CHRIS DEOTTE's notebook. Thank you!
### https://www.kaggle.com/code/cdeotte/candidate-rerank-model-lb-0-575
---

# Candidate ReRank Model using Handcrafted Rules
In this notebook, we present a "candidate rerank" model using handcrafted rules. We can improve this model by engineering features, merging them unto items and users, and training a reranker model (such as XGB) to choose our final 20. Furthermore to tune and improve this notebook, we should build a local CV scheme to experiment new logic and/or models.

 UPDATE: I published a notebook to compute validation score [here][10] using Radek's scheme described [here][11].

Note in this competition, a "session" actually means a unique "user". So our task is to predict what each of the `1,671,803` test "users" (i.e. "sessions") will do in the future. For each test "user" (i.e. "session") we must predict what they will `click`, `cart`, and `order` during the remainder of the week long test period.

### Step 1 - Generate Candidates
For each test user, we generate possible choices, i.e. candidates. In this notebook, we generate candidates from 5 sources:
* User history of clicks, carts, orders
* Most popular 20 clicks, carts, orders during test week
* Co-visitation matrix of click/cart/order to cart/order with type weighting
* Co-visitation matrix of cart/order to cart/order called buy2buy
* Co-visitation matrix of click/cart/order to clicks with time weighting

### Step 2 - ReRank and Choose 20
Given the list of candidates, we must select 20 to be our predictions. In this notebook, we do this with a set of handcrafted rules. We can improve our predictions by training an XGBoost model to select for us. Our handcrafted rules give priority to:
* Most recent previously visited items
* Items previously visited multiple times
* Items previously in cart or order
* Co-visitation matrix of cart/order to cart/order
* Current popular items

![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Nov-2022/c_r_model.png)
  
# Credits
We thank many Kagglers who have shared ideas. We use co-visitation matrix idea from Vladimir [here][1]. We use groupby sort logic from Sinan in comment section [here][4]. We use duplicate prediction removal logic from Radek [here][5]. We use multiple visit logic from Pietro [here][2]. We use type weighting logic from Ingvaras [here][3]. We use leaky test data from my previous notebook [here][4]. And some ideas may have originated from Tawara [here][6] and KJ [here][7]. We use Colum2131's parquets [here][8]. Above image is from Ravi's discussion about candidate rerank models [here][9]

[1]: https://www.kaggle.com/code/vslaykovsky/co-visitation-matrix
[2]: https://www.kaggle.com/code/pietromaldini1/multiple-clicks-vs-latest-items
[3]: https://www.kaggle.com/code/ingvarasgalinskas/item-type-vs-multiple-clicks-vs-latest-items
[4]: https://www.kaggle.com/code/cdeotte/test-data-leak-lb-boost
[5]: https://www.kaggle.com/code/radek1/co-visitation-matrix-simplified-imprvd-logic
[6]: https://www.kaggle.com/code/ttahara/otto-mors-aid-frequency-baseline
[7]: https://www.kaggle.com/code/whitelily/co-occurrence-baseline
[8]: https://www.kaggle.com/datasets/columbia2131/otto-chunk-data-inparquet-format
[9]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364721
[10]: https://www.kaggle.com/cdeotte/compute-validation-score-cv-564
[11]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364991

# Step 1 - Candidate Generation with RAPIDS
For candidate generation, we build three co-visitation matrices. One computes the popularity of cart/order given a user's previous click/cart/order. We apply type weighting to this matrix. One computes the popularity of cart/order given a user's previous cart/order. We call this "buy2buy" matrix. One computes the popularity of clicks given a user previously click/cart/order.  We apply time weighting to this matrix. We will use RAPIDS cuDF GPU to compute these matrices quickly!
"""

import pandas as pd, numpy as np
from tqdm.notebook import tqdm
import os, sys, pickle, glob, gc
from collections import Counter
import cudf, itertools
print('We will use RAPIDS version',cudf.__version__)

"""## Compute Three Co-visitation Matrices with RAPIDS
We will compute 3 co-visitation matrices using RAPIDS cuDF on GPU. This is 30x faster than using Pandas CPU like other public notebooks! For maximum speed, set the variable `DISK_PIECES` to the smallest number possible based on the GPU you are using without incurring memory errors. If you run this code offline with 32GB GPU ram, then you can use `DISK_PIECES = 1` and compute each co-visitation matrix in almost 1 minute! Kaggle's GPU only has 16GB ram, so we use `DISK_PIECES = 4` and it takes an amazing 3 minutes each! Below are some of the tricks to speed up computation
* Use RAPIDS cuDF GPU instead of Pandas CPU
* Read disk once and save in CPU RAM for later GPU multiple use
* Process largest amount of data possible on GPU at one time
* Merge data in two stages. Multiple small to single medium. Multiple medium to single large.
* Write result as parquet instead of dictionary
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # CACHE FUNCTIONS
# def read_file(f):
#     return cudf.DataFrame( data_cache[f] )
# def read_file_to_cache(f):
#     df = pd.read_parquet(f)
#     df.ts = (df.ts/1000).astype('int32')
#     df['type'] = df['type'].map(type_labels).astype('int8')
#     return df
# 
# # CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU
# data_cache = {}
# type_labels = {'clicks':0, 'carts':1, 'orders':2}
# files = glob.glob('../input/otto-chunk-data-inparquet-format/*_parquet/*')
# for f in files: data_cache[f] = read_file_to_cache(f)
# 
# # CHUNK PARAMETERS
# READ_CT = 5
# CHUNK = int( np.ceil( len(files)/6 ))
# print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')

"""## 1) "Carts Orders" Co-visitation Matrix - Type Weighted"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR
# DISK_PIECES = 4
# SIZE = 1.86e6/DISK_PIECES
# 
# # COMPUTE IN PARTS FOR MEMORY MANGEMENT
# for PART in range(DISK_PIECES):
#     print()
#     print('### DISK PART',PART+1)
#     
#     # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS
#     # => OUTER CHUNKS
#     for j in range(6):
#         a = j*CHUNK
#         b = min( (j+1)*CHUNK, len(files) )
#         print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')
#         
#         # => INNER CHUNKS
#         for k in range(a,b,READ_CT):
#             # READ FILE
#             df = [read_file(files[k])]
#             for i in range(1,READ_CT): 
#                 if k+i<b: df.append( read_file(files[k+i]) )
#             df = cudf.concat(df,ignore_index=True,axis=0)
#             df = df.sort_values(['session','ts'],ascending=[True,False])
#             
#             # USE TAIL OF SESSION
#             df = df.reset_index(drop=True)
#             df['n'] = df.groupby('session').cumcount()
#             df = df.loc[df.n<30].drop('n',axis=1)
#             
#             # CREATE PAIRS
#             df = df.merge(df,on='session')
#             df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]
#             
#             # MEMORY MANAGEMENT COMPUTE IN PARTS
#             df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]
#             
#             # ASSIGN WEIGHTS
#             df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])
#             df['wgt'] = df.type_y.map(type_weight)
#             df = df[['aid_x','aid_y','wgt']]
#             df.wgt = df.wgt.astype('float32')
#             df = df.groupby(['aid_x','aid_y']).wgt.sum()
#             
#             # COMBINE INNER CHUNKS
#             if k==a: tmp2 = df
#             else: tmp2 = tmp2.add(df, fill_value=0)
#             print(k,', ',end='')
#         
#         print()
#         
#         # COMBINE OUTER CHUNKS
#         if a==0: tmp = tmp2
#         else: tmp = tmp.add(tmp2, fill_value=0)
#         del tmp2, df
#         gc.collect()
# 
#     # CONVERT MATRIX TO DICTIONARY
#     tmp = tmp.reset_index()
#     tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])
#     
#     # SAVE TOP 40
#     tmp = tmp.reset_index(drop=True)
#     tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()
#     tmp = tmp.loc[tmp.n<carts_th].drop('n',axis=1)
#     
#     # SAVE PART TO DISK (convert to pandas first uses less memory)
#     tmp.to_pandas().to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')

"""## 2) "Buy2Buy" Co-visitation Matrix"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR
# DISK_PIECES = 1
# SIZE = 1.86e6/DISK_PIECES
# 
# # COMPUTE IN PARTS FOR MEMORY MANGEMENT
# for PART in range(DISK_PIECES):
#     print()
#     print('### DISK PART',PART+1)
#     
#     # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS
#     # => OUTER CHUNKS
#     for j in range(6):
#         a = j*CHUNK
#         b = min( (j+1)*CHUNK, len(files) )
#         print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')
#         
#         # => INNER CHUNKS
#         for k in range(a,b,READ_CT):
#             
#             # READ FILE
#             df = [read_file(files[k])]
#             for i in range(1,READ_CT): 
#                 if k+i<b: df.append( read_file(files[k+i]) )
#             df = cudf.concat(df,ignore_index=True,axis=0)
#             df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS
#             df = df.sort_values(['session','ts'],ascending=[True,False])
#             
#             # USE TAIL OF SESSION
#             df = df.reset_index(drop=True)
#             df['n'] = df.groupby('session').cumcount()
#             df = df.loc[df.n<30].drop('n',axis=1)
#             
#             # CREATE PAIRS
#             df = df.merge(df,on='session')
#             df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS
#             
#             # MEMORY MANAGEMENT COMPUTE IN PARTS
#             df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]
#             
#             # ASSIGN WEIGHTS
#             df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])
#             df['wgt'] = 1
#             df = df[['aid_x','aid_y','wgt']]
#             df.wgt = df.wgt.astype('float32')
#             df = df.groupby(['aid_x','aid_y']).wgt.sum()
#             
#             # COMBINE INNER CHUNKS
#             if k==a: tmp2 = df
#             else: tmp2 = tmp2.add(df, fill_value=0)
#             print(k,', ',end='')
# 
#         print()
#         
#         # COMBINE OUTER CHUNKS
#         if a==0: tmp = tmp2
#         else: tmp = tmp.add(tmp2, fill_value=0)
#         del tmp2, df
#         gc.collect()
# 
#     # CONVERT MATRIX TO DICTIONARY
#     tmp = tmp.reset_index()
#     tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])
#     
#     # SAVE TOP 40
#     tmp = tmp.reset_index(drop=True)
#     tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()
#     tmp = tmp.loc[tmp.n<orders_th].drop('n',axis=1)
#     
#     # SAVE PART TO DISK (convert to pandas first uses less memory)
#     tmp.to_pandas().to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')

"""## 3) "Clicks" Co-visitation Matrix - Time Weighted"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR
# DISK_PIECES = 4
# SIZE = 1.86e6/DISK_PIECES
# 
# # COMPUTE IN PARTS FOR MEMORY MANGEMENT
# for PART in range(DISK_PIECES):
#     print()
#     print('### DISK PART',PART+1)
#     
#     # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS
#     # => OUTER CHUNKS
#     for j in range(6):
#         a = j*CHUNK
#         b = min( (j+1)*CHUNK, len(files) )
#         print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')
#         
#         # => INNER CHUNKS
#         for k in range(a,b,READ_CT):
#             # READ FILE
#             df = [read_file(files[k])]
#             for i in range(1,READ_CT): 
#                 if k+i<b: df.append( read_file(files[k+i]) )
#             df = cudf.concat(df,ignore_index=True,axis=0)
#             df = df.sort_values(['session','ts'],ascending=[True,False])
#             
#             # USE TAIL OF SESSION
#             df = df.reset_index(drop=True)
#             df['n'] = df.groupby('session').cumcount()
#             df = df.loc[df.n<30].drop('n',axis=1)
#             
#             # CREATE PAIRS
#             df = df.merge(df,on='session')
#             df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]
#             
#             # MEMORY MANAGEMENT COMPUTE IN PARTS
#             df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]
#             
#             # ASSIGN WEIGHTS
#             df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])
#             df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)
#             # 1659304800 : minimum timestamp
#             # 1662328791 : maximum timestamp
#             df = df[['aid_x','aid_y','wgt']]
#             df.wgt = df.wgt.astype('float32')
#             df = df.groupby(['aid_x','aid_y']).wgt.sum()
#             
#             # COMBINE INNER CHUNKS
#             if k==a: tmp2 = df
#             else: tmp2 = tmp2.add(df, fill_value=0)
#             print(k,', ',end='')
#         print()
#         
#         # COMBINE OUTER CHUNKS
#         if a==0: tmp = tmp2
#         else: tmp = tmp.add(tmp2, fill_value=0)
#         del tmp2, df
#         gc.collect()
# 
#     # CONVERT MATRIX TO DICTIONARY
#     tmp = tmp.reset_index()
#     tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])
#     
#     # SAVE TOP 40
#     tmp = tmp.reset_index(drop=True)
#     tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()
#     tmp = tmp.loc[tmp.n<clicks_th].drop('n',axis=1)
#     
#     # SAVE PART TO DISK (convert to pandas first uses less memory)
#     tmp.to_pandas().to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')

# FREE MEMORY
del data_cache, tmp
_ = gc.collect()

"""# Step 2 - ReRank (choose 20) using handcrafted rules
For description of the handcrafted rules, read this notebook's intro.
"""

def load_test():    
    dfs = []
    for e, chunk_file in enumerate(glob.glob('../input/otto-chunk-data-inparquet-format/test_parquet/*')):
        chunk = pd.read_parquet(chunk_file)
        chunk.ts = (chunk.ts/1000).astype('int32')
        chunk['type'] = chunk['type'].map(type_labels).astype('int8')
        dfs.append(chunk)
    return pd.concat(dfs).reset_index(drop=True) #.astype({"ts": "datetime64[ms]"})

test_df = load_test()
print('Test data has shape',test_df.shape)
test_df.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# def pqt_to_dict(df):
#     return df.groupby('aid_x').aid_y.apply(list).to_dict()
# 
# # LOAD THREE CO-VISITATION MATRICES
# top_20_clicks = pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_0.pqt') )
# 
# for k in range(1,DISK_PIECES): 
#     top_20_clicks.update( pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_{k}.pqt') ) )
# 
# 
# top_20_buys = pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_0.pqt') )
# 
# for k in range(1,DISK_PIECES): 
#     top_20_buys.update( pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_{k}.pqt') ) )
# 
# top_20_buy2buy = pqt_to_dict( pd.read_parquet(f'top_15_buy2buy_v{VER}_0.pqt') )
# 
# # TOP CLICKS AND ORDERS IN TEST
# #top_clicks = test_df.loc[test_df['type']=='clicks','aid'].value_counts().index.values[:20]
# #top_orders = test_df.loc[test_df['type']=='orders','aid'].value_counts().index.values[:20]
# 
# print('Here are size of our 3 co-visitation matrices:')
# print( len( top_20_clicks ), len( top_20_buy2buy ), len( top_20_buys ) )

top_clicks = test_df.loc[test_df['type']== 0,'aid'].value_counts().index.values[:20] 
top_carts = test_df.loc[test_df['type']== 1,'aid'].value_counts().index.values[:20]
top_orders = test_df.loc[test_df['type']== 2,'aid'].value_counts().index.values[:20]

def suggest_clicks(df):
    # USER HISTORY AIDS AND TYPES
    aids=df.aid.tolist()
    types = df.type.tolist()
    unique_aids = list(dict.fromkeys(aids[::-1] ))
    # RERANK CANDIDATES USING WEIGHTS
    if len(unique_aids)>=20:
        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1
        aids_temp = Counter() 
        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS
        for aid,w,t in zip(aids,weights,types): 
            aids_temp[aid] += w * type_weight_multipliers[t]
        sorted_aids = [k for k,v in aids_temp.most_common(20)]
        return sorted_aids
    # USE "CLICKS" CO-VISITATION MATRIX
    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))
    # RERANK CANDIDATES
    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(20) if aid2 not in unique_aids]    
    result = unique_aids + top_aids2[:20 - len(unique_aids)]
    # USE TOP20 TEST CLICKS
    return result + list(top_clicks)[:20-len(result)]

def suggest_carts(df):
    # User history aids and types
    aids = df.aid.tolist()
    types = df.type.tolist()
    
    # UNIQUE AIDS AND UNIQUE BUYS
    unique_aids = list(dict.fromkeys(aids[::-1] ))
    df = df.loc[(df['type'] == 0)|(df['type'] == 1)]
    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))
    
    # Rerank candidates using weights
    if len(unique_aids) >= 20:
        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1
        aids_temp = Counter() 
        
        # Rerank based on repeat items and types of items
        for aid,w,t in zip(aids,weights,types): 
            aids_temp[aid] += w * type_weight_multipliers[t]
        
        # Rerank candidates using"top_20_carts" co-visitation matrix
        aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_buys if aid in top_20_buys]))
        for aid in aids2: aids_temp[aid] += 0.1
        sorted_aids = [k for k,v in aids_temp.most_common(20)]
        return sorted_aids
    
    # Use "cart order" and "clicks" co-visitation matrices
    aids1 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))
    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))
    
    # RERANK CANDIDATES
    top_aids2 = [aid2 for aid2, cnt in Counter(aids1+aids2).most_common(20) if aid2 not in unique_aids] 
    result = unique_aids + top_aids2[:20 - len(unique_aids)]
    
    # USE TOP20 TEST ORDERS
    return result + list(top_carts)[:20-len(result)]

def suggest_buys(df):
    # USER HISTORY AIDS AND TYPES
    aids=df.aid.tolist()
    types = df.type.tolist()
    # UNIQUE AIDS AND UNIQUE BUYS
    unique_aids = list(dict.fromkeys(aids[::-1] ))
    df = df.loc[(df['type']==1)|(df['type']==2)]
    unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))
    # RERANK CANDIDATES USING WEIGHTS
    if len(unique_aids)>=20:
        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1
        aids_temp = Counter() 
        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS
        for aid,w,t in zip(aids,weights,types): 
            aids_temp[aid] += w * type_weight_multipliers[t]
        # RERANK CANDIDATES USING "BUY2BUY" CO-VISITATION MATRIX
        aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))
        for aid in aids3: aids_temp[aid] += 0.1
        sorted_aids = [k for k,v in aids_temp.most_common(20)]
        return sorted_aids
    # USE "CART ORDER" CO-VISITATION MATRIX
    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))
    # USE "BUY2BUY" CO-VISITATION MATRIX
    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))
    # RERANK CANDIDATES
    top_aids2 = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(20) if aid2 not in unique_aids] 
    result = unique_aids + top_aids2[:20 - len(unique_aids)]
    # USE TOP20 TEST ORDERS
    return result + list(top_orders)[:20-len(result)]

"""# Create Submission CSV
Inferring test data with Pandas groupby is slow. We need to accelerate the following code.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# pred_df_clicks = test_df.sort_values(["session", "ts"]).groupby(["session"]).apply(
#     lambda x: suggest_clicks(x)
# )
# 
# pred_df_carts = test_df.sort_values(["session", "ts"]).groupby(["session"]).apply(
#     lambda x: suggest_carts(x)
# )
# 
# pred_df_buys = test_df.sort_values(["session", "ts"]).groupby(["session"]).apply(
#     lambda x: suggest_buys(x)
# )

clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix("_clicks"), columns=["labels"]).reset_index()
orders_pred_df = pd.DataFrame(pred_df_buys.add_suffix("_orders"), columns=["labels"]).reset_index()
carts_pred_df = pd.DataFrame(pred_df_carts.add_suffix("_carts"), columns=["labels"]).reset_index()

pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])
pred_df.columns = ["session_type", "labels"]
pred_df["labels"] = pred_df.labels.apply(lambda x: " ".join(map(str,x)))
pred_df.to_csv("submission.csv", index=False)
pred_df.head()